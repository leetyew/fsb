{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2 & Table 3: Aggregation\n",
    "\n",
    "Per plan Part E: Notebooks ONLY aggregate. No training, no evaluation, no branching.\n",
    "\n",
    "- Table 2 (Exp I): RMSE = sqrt(mean(error_r^2)) where error_r = estimate_r - truth_r\n",
    "- Table 3 (Exp II): mean +/- SE where SE = std / sqrt(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results\n",
    "\n",
    "Update the paths below to point to your experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these paths\n",
    "EXP1_PATH = \"../experiments/exp1_YYYYMMDD/exp1_results.csv\"\n",
    "EXP2_PATH = \"../experiments/exp2_YYYYMMDD/exp2_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Experiment I results\n",
    "try:\n",
    "    exp1_df = pd.read_csv(EXP1_PATH)\n",
    "    print(f\"Exp I: {len(exp1_df)} rows loaded\")\n",
    "    display(exp1_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {EXP1_PATH}\")\n",
    "    exp1_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Experiment II results\n",
    "try:\n",
    "    exp2_df = pd.read_csv(EXP2_PATH)\n",
    "    print(f\"Exp II: {len(exp2_df)} rows loaded\")\n",
    "    display(exp2_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {EXP2_PATH}\")\n",
    "    exp2_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2: Evaluation Accuracy (Exp I)\n",
    "\n",
    "For each (method, metric): RMSE = sqrt(mean((estimate - truth)^2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_table2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute Table 2 aggregation.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"error\"] = df[\"estimate\"] - df[\"truth\"]\n",
    "    df[\"sq_error\"] = df[\"error\"] ** 2\n",
    "    \n",
    "    # Group by method and metric\n",
    "    grouped = df.groupby([\"method\", \"metric\"]).agg(\n",
    "        rmse=(\"sq_error\", lambda x: np.sqrt(x.mean())),\n",
    "        bias=(\"error\", \"mean\"),\n",
    "        n_replicates=(\"replicate_id\", \"count\"),\n",
    "    ).reset_index()\n",
    "    \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp1_df is not None:\n",
    "    table2 = compute_table2(exp1_df)\n",
    "    print(\"\\n=== TABLE 2: Evaluation Accuracy ===\")\n",
    "    display(table2.pivot(index=\"method\", columns=\"metric\", values=\"rmse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3: Training Comparison (Exp II)\n",
    "\n",
    "For each (train_method, metric): mean +/- SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_table3(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute Table 3 aggregation.\"\"\"\n",
    "    grouped = df.groupby([\"train_method\", \"metric\"]).agg(\n",
    "        mean=(\"value\", \"mean\"),\n",
    "        std=(\"value\", \"std\"),\n",
    "        n_replicates=(\"replicate_id\", \"count\"),\n",
    "    ).reset_index()\n",
    "    \n",
    "    grouped[\"se\"] = grouped[\"std\"] / np.sqrt(grouped[\"n_replicates\"])\n",
    "    grouped[\"formatted\"] = grouped.apply(\n",
    "        lambda r: f\"{r['mean']:.4f} +/- {r['se']:.4f}\", axis=1\n",
    "    )\n",
    "    \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp2_df is not None:\n",
    "    table3 = compute_table3(exp2_df)\n",
    "    print(\"\\n=== TABLE 3: Training Comparison ===\")\n",
    "    display(table3.pivot(index=\"train_method\", columns=\"metric\", values=\"formatted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp1_df is not None:\n",
    "    print(\"Exp I Summary:\")\n",
    "    print(f\"  Methods: {exp1_df['method'].unique().tolist()}\")\n",
    "    print(f\"  Metrics: {exp1_df['metric'].unique().tolist()}\")\n",
    "    print(f\"  Replicates: {exp1_df['replicate_id'].nunique()}\")\n",
    "\n",
    "if exp2_df is not None:\n",
    "    print(\"\\nExp II Summary:\")\n",
    "    print(f\"  Methods: {exp2_df['train_method'].unique().tolist()}\")\n",
    "    print(f\"  Metrics: {exp2_df['metric'].unique().tolist()}\")\n",
    "    print(f\"  Replicates: {exp2_df['replicate_id'].nunique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
