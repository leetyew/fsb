{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Figures: Fighting Sampling Bias\n",
    "\n",
    "This notebook recreates all figures and tables from the paper:\n",
    "\n",
    "**Figures:**\n",
    "- Figure 2: Loss due to sampling bias (5 panels: a-e)\n",
    "- Figure 3: BASL sensitivity analysis  \n",
    "- Figure 4: Feature bias analysis\n",
    "\n",
    "**Tables:**\n",
    "- Table 1: Bayesian evaluation comparison\n",
    "- Table 2: BASL training effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Style settings for paper-quality figures\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 11,\n",
    "    'legend.fontsize': 9,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'figure.dpi': 100,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "})\n",
    "\n",
    "EXPERIMENTS_DIR = Path('../experiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_latest_experiment(prefix: str) -> Tuple[Path, dict]:\n    \"\"\"Load the most recent experiment matching a prefix.\"\"\"\n    exp_dirs = sorted(EXPERIMENTS_DIR.glob(f\"{prefix}*\"), reverse=True)\n    if not exp_dirs:\n        raise FileNotFoundError(f\"No experiments found with prefix: {prefix}\")\n    \n    exp_dir = exp_dirs[0]\n    print(f\"Loading experiment: {exp_dir.name}\")\n    return exp_dir, load_experiment(exp_dir)\n\n\ndef load_experiment(exp_dir: Path) -> dict:\n    \"\"\"Load all data from an experiment directory.\"\"\"\n    data = {}\n    \n    # Load metrics history if available\n    metrics_files = list(exp_dir.glob(\"metrics_history_*.json\"))\n    if metrics_files:\n        with open(metrics_files[0]) as f:\n            data['metrics_history'] = json.load(f)\n    \n    # Load aggregated results if available\n    agg_file = exp_dir / \"aggregated.json\"\n    if agg_file.exists():\n        with open(agg_file) as f:\n            data['aggregated'] = json.load(f)\n    \n    # Load Figure 2 data if available (Experiment 1: panels a-c)\n    figure2_files = list(exp_dir.glob(\"figure2_data_*.json\"))\n    if figure2_files:\n        with open(figure2_files[0]) as f:\n            data['figure2_data'] = json.load(f)\n    \n    # Load BASL-LR data if available (Experiment 2: for panels b-c bridge)\n    basl_lr_file = exp_dir / \"basl_lr_data.json\"\n    if basl_lr_file.exists():\n        with open(basl_lr_file) as f:\n            data['basl_lr_data'] = json.load(f)\n    \n    # Load feature bias analysis if available (Experiment 2 only)\n    bias_file = exp_dir / \"feature_bias_analysis.json\"\n    if bias_file.exists():\n        with open(bias_file) as f:\n            data['feature_bias'] = json.load(f)\n    \n    return data\n\n\ndef extract_metric_series(metrics_history: list, metric: str = 'abr') -> dict:\n    \"\"\"Extract a metric series from metrics history for all model types.\"\"\"\n    iterations = [m['iteration'] for m in metrics_history]\n    \n    series = {'iteration': iterations}\n    \n    model_types = ['oracle', 'model_holdout', 'accepts', 'bayesian']\n    for model_type in model_types:\n        if model_type in metrics_history[0]:\n            series[model_type] = [m[model_type][metric] for m in metrics_history]\n    \n    return series"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Experiment 1: Bayesian Evaluation (Figure 2 panels a-d)\n",
    "try:\n",
    "    exp1_dir, exp1_data = load_latest_experiment('exp1_bayesian_eval')\n",
    "    print(f\"Exp 1 keys: {list(exp1_data.keys())}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    exp1_data = None\n",
    "\n",
    "# Load Experiment 2: BASL Training (Figure 2 panel e, Figures 3-4, Table 2)\n",
    "try:\n",
    "    exp2_dir, exp2_data = load_latest_experiment('exp2_basl_training')\n",
    "    print(f\"Exp 2 keys: {list(exp2_data.keys())}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    exp2_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Figure 2: Loss Due to Sampling Bias\n\nFive panels showing how sampling bias propagates:\n- **(a) Bias in Data**: Feature distributions (Population vs Accepts vs Rejects)\n- **(b) Bias in Model**: LR Coefficients (Accepts-only vs Oracle vs BASL) - 2 features for interpretability\n- **(c) Bias in Predictions**: LR Score distributions (Accepts-only vs Oracle vs BASL)\n- **(d) Impact on Evaluation**: ABR over iterations (Bayesian vs Accepts-only) - Experiment I core result\n- **(e) Impact on Training**: ABR over iterations (BASL vs Accepts-only) - Experiment II dynamics\n\n**Note:** Panels (b) and (c) are \"bridge panels\" using Logistic Regression for interpretability.\nBASL-LR data comes from Experiment 2 and is combined with Experiment 1 data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_figure_2(exp1_data: dict, exp2_data: dict = None):\n    \"\"\"Plot Figure 2: Complete 5-panel visualization.\n    \n    Panels (a), (d): From Experiment 1\n    Panels (b), (c): Bridge panels - Exp1 (Accepts, Oracle LR) + Exp2 (BASL-LR)\n    Panel (e): From Experiment 2\n    \"\"\"\n    \n    fig = plt.figure(figsize=(16, 10))\n    gs = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.35)\n    \n    # Panel (a): Bias in Data - x_v (most separating feature, typically X1) distributions\n    ax_a = fig.add_subplot(gs[0, 0])\n    if exp1_data and 'figure2_data' in exp1_data:\n        panel_a = exp1_data['figure2_data']['panel_a']\n        x_v_feature = panel_a.get('x_v_feature', 'X1')  # Paper-faithful: X1\n        \n        # Plot density histograms for x_v (most separating feature)\n        bins = 50\n        ax_a.hist(panel_a['population_x_v'], bins=bins, alpha=0.4, \n                  label='Population', density=True, color='gray')\n        ax_a.hist(panel_a['accepts_x_v'], bins=bins, alpha=0.5, \n                  label='Accepts', density=True, color='blue')\n        ax_a.hist(panel_a['rejects_x_v'], bins=bins, alpha=0.5, \n                  label='Rejects', density=True, color='red')\n        \n        ax_a.set_xlabel(f'Feature {x_v_feature} (lower = lower risk)')\n        ax_a.set_ylabel('Density')\n        ax_a.set_title('(a) Bias in Data')\n        ax_a.legend()\n    \n    # Panel (b): Bias in Model - LR coefficients (paper: Intercept, X1, X2, N1, N2)\n    # Three bars per feature: Accepts, Oracle, BASL\n    ax_b = fig.add_subplot(gs[0, 1])\n    if exp1_data and 'figure2_data' in exp1_data:\n        panel_b = exp1_data['figure2_data']['panel_b']\n\n        # Paper-faithful x-axis: Intercept, X1, X2, N1, N2\n        feature_names = panel_b['feature_names']\n\n        # Build coefficient arrays: [intercept, coef1, coef2, ...]\n        accepts_coefs = [panel_b['accepts_intercept']] + panel_b['accepts_coefficients']\n        oracle_coefs = [panel_b['oracle_intercept']] + panel_b['oracle_coefficients']\n\n        # Check if BASL-LR data available from Exp 2 with matching features\n        basl_coefs = None\n        if exp2_data and 'basl_lr_data' in exp2_data:\n            basl_lr = exp2_data['basl_lr_data'][0]  # First seed\n            basl_coefs_raw = [basl_lr['basl_intercept']] + basl_lr['basl_coefficients']\n            # Only use BASL coefs if they match Exp1 feature count\n            if len(basl_coefs_raw) == len(feature_names):\n                basl_coefs = basl_coefs_raw\n            else:\n                print(f\"Warning: BASL has {len(basl_coefs_raw)} coefs but Exp1 has {len(feature_names)} features. Skipping BASL in panel (b).\")\n\n        x_pos = np.arange(len(feature_names))\n        width = 0.25 if basl_coefs else 0.35\n\n        ax_b.bar(x_pos - width, accepts_coefs, width,\n                 label='Accepts-only', alpha=0.7, color='red')\n        ax_b.bar(x_pos, oracle_coefs, width,\n                 label='Oracle', alpha=0.7, color='green')\n        if basl_coefs:\n            ax_b.bar(x_pos + width, basl_coefs, width,\n                     label='BASL', alpha=0.7, color='blue')\n\n        ax_b.set_xlabel('Coefficient')\n        ax_b.set_ylabel('Value')\n        ax_b.set_title('(b) Bias in Model (LR surrogate)')\n        ax_b.set_xticks(x_pos)\n        ax_b.set_xticklabels(feature_names, rotation=45, ha='right')\n        ax_b.legend()\n        ax_b.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n    \n    # Panel (c): Bias in Predictions - LR score distributions (3 models)\n    ax_c = fig.add_subplot(gs[0, 2])\n    if exp1_data and 'figure2_data' in exp1_data:\n        panel_c = exp1_data['figure2_data']['panel_c']\n        \n        bins = 50\n        ax_c.hist(panel_c['accepts_scores'], bins=bins, alpha=0.5,\n                  label='Accepts-only', density=True, color='red')\n        ax_c.hist(panel_c['oracle_scores'], bins=bins, alpha=0.5,\n                  label='Oracle', density=True, color='green')\n        \n        # Add BASL scores if available from Exp 2\n        if exp2_data and 'basl_lr_data' in exp2_data:\n            basl_lr = exp2_data['basl_lr_data'][0]\n            ax_c.hist(basl_lr['basl_scores'], bins=bins, alpha=0.5,\n                      label='BASL', density=True, color='blue')\n        \n        ax_c.set_xlabel('Predicted Score (P(bad))')\n        ax_c.set_ylabel('Density')\n        ax_c.set_title('(c) Bias in Predictions (LR)')\n        ax_c.legend()\n    \n    # Panel (d): Impact on Evaluation - Experiment I core result\n    ax_d = fig.add_subplot(gs[1, 0:2])\n    if exp1_data and 'metrics_history' in exp1_data:\n        series = extract_metric_series(exp1_data['metrics_history'], 'abr')\n        \n        ax_d.plot(series['iteration'], series['oracle'], 'k-', \n                 linewidth=2, label='Oracle', marker='o', markersize=3, markevery=10)\n        ax_d.plot(series['iteration'], series['accepts'], 'r--', \n                 linewidth=2, label='Accepts-only', marker='s', markersize=3, markevery=10)\n        ax_d.plot(series['iteration'], series['bayesian'], 'b-', \n                 linewidth=2, label='Bayesian', marker='^', markersize=3, markevery=10)\n        \n        ax_d.set_xlabel('Acceptance Loop Iteration')\n        ax_d.set_ylabel('ABR (Average Bad Rate)')\n        ax_d.set_title('(d) Impact on Evaluation: Bayesian vs Accepts-only (Exp I)')\n        ax_d.legend(loc='best')\n        ax_d.set_ylim(0, 0.8)\n        ax_d.grid(True, alpha=0.3)\n    \n    # Panel (e): Impact on Training - Experiment II dynamics\n    ax_e = fig.add_subplot(gs[1, 2])\n    if exp2_data and 'metrics_history' in exp2_data:\n        series = extract_metric_series(exp2_data['metrics_history'], 'abr')\n        \n        # Oracle line (constant)\n        ax_e.plot(series['iteration'], series['oracle'], 'k-', \n                 linewidth=2, label='Oracle', marker='o', markersize=3, markevery=10)\n        \n        # Accepts-only baseline (constant reference from aggregated data)\n        if 'aggregated' in exp2_data:\n            agg = exp2_data['aggregated']\n            baseline_key = 'xgb_accepts_abr_mean' if 'xgb_accepts_abr_mean' in agg else 'accepts_abr_mean'\n            baseline_abr = agg[baseline_key]\n            ax_e.axhline(y=baseline_abr, color='r', linestyle='--', \n                        linewidth=2, label='Accepts-only (baseline)')\n        \n        # BASL model evolution\n        if 'model_holdout' in series:\n            ax_e.plot(series['iteration'], series['model_holdout'], 'b-', \n                     linewidth=2, label='BASL', marker='^', markersize=3, markevery=10)\n        \n        ax_e.set_xlabel('Iteration')\n        ax_e.set_ylabel('ABR')\n        ax_e.set_title('(e) Impact on Training:\\nBASL vs Accepts-only (Exp II)')\n        ax_e.legend(loc='best', fontsize=8)\n        ax_e.set_ylim(0, 0.8)\n        ax_e.grid(True, alpha=0.3)\n    \n    plt.suptitle('Figure 2: Loss Due to Sampling Bias', fontsize=14, fontweight='bold', y=0.995)\n    plt.show()\n    \n    return fig\n\n# Plot Figure 2\nfig2 = plot_figure_2(exp1_data, exp2_data)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Table 1: Bayesian Evaluation Reliability\n",
    "\n",
    "RMSE between estimated and oracle (true) metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(true_values: list, estimated_values: list) -> float:\n",
    "    \"\"\"Compute Root Mean Square Error.\"\"\"\n",
    "    true_arr = np.array(true_values)\n",
    "    est_arr = np.array(estimated_values)\n",
    "    return np.sqrt(np.mean((true_arr - est_arr) ** 2))\n",
    "\n",
    "\n",
    "def create_table_1(exp1_data: dict) -> pd.DataFrame:\n",
    "    \"\"\"Create Table 1: Evaluation method comparison.\"\"\"\n",
    "    \n",
    "    if not exp1_data or 'metrics_history' not in exp1_data:\n",
    "        print(\"Experiment 1 data not available\")\n",
    "        return None\n",
    "    \n",
    "    metrics_history = exp1_data['metrics_history']\n",
    "    \n",
    "    # Skip first iteration (initialization)\n",
    "    metrics_history = [m for m in metrics_history if m['iteration'] > 0]\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = ['auc', 'brier', 'abr']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method in ['accepts', 'bayesian']:\n",
    "        results[method] = {}\n",
    "        for metric in metrics:\n",
    "            oracle_vals = [m['oracle'][metric] for m in metrics_history]\n",
    "            method_vals = [m[method][metric] for m in metrics_history]\n",
    "            rmse = compute_rmse(oracle_vals, method_vals)\n",
    "            results[method][f'{metric}_rmse'] = rmse\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results).T\n",
    "    df.index.name = 'Method'\n",
    "    df.index = ['Accepts-only', 'Bayesian']\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    df.columns = ['AUC RMSE', 'Brier RMSE', 'ABR RMSE']\n",
    "    \n",
    "    # Format values\n",
    "    df = df.round(4)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Create and display Table 1\n",
    "table_1 = create_table_1(exp1_data)\n",
    "if table_1 is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Table 1: Bayesian Evaluation - RMSE vs Oracle\")\n",
    "    print(\"=\"*70)\n",
    "    print(table_1.to_string())\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nInterpretation: Lower RMSE = better estimation of true (oracle) performance.\")\n",
    "    print(\"Bayesian evaluation should show lower RMSE than accepts-only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Table 2: BASL Training Effectiveness\n",
    "\n",
    "Comparison of final model performance (AUC and ABR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_table_2(exp2_data: dict) -> pd.DataFrame:\n    \"\"\"Create Table 2: Training method comparison.\"\"\"\n    \n    if not exp2_data or 'aggregated' not in exp2_data:\n        print(\"Experiment 2 aggregated data not available\")\n        return None\n    \n    agg = exp2_data['aggregated']\n    \n    # Handle both key naming conventions (xgb_accepts_* or accepts_*)\n    accepts_auc_key = 'xgb_accepts_auc_mean' if 'xgb_accepts_auc_mean' in agg else 'accepts_auc_mean'\n    accepts_auc_std_key = 'xgb_accepts_auc_std' if 'xgb_accepts_auc_std' in agg else 'accepts_auc_std'\n    accepts_abr_key = 'xgb_accepts_abr_mean' if 'xgb_accepts_abr_mean' in agg else 'accepts_abr_mean'\n    accepts_abr_std_key = 'xgb_accepts_abr_std' if 'xgb_accepts_abr_std' in agg else 'accepts_abr_std'\n    \n    # Extract final metrics\n    data = {\n        'Method': ['Oracle', 'Accepts-only', 'BASL'],\n        'AUC': [\n            f\"{agg['oracle_auc_mean']:.4f} +/- {agg['oracle_auc_std']:.4f}\",\n            f\"{agg[accepts_auc_key]:.4f} +/- {agg[accepts_auc_std_key]:.4f}\",\n            f\"{agg['basl_auc_mean']:.4f} +/- {agg['basl_auc_std']:.4f}\",\n        ],\n        'ABR': [\n            f\"{agg['oracle_abr_mean']:.4f} +/- {agg['oracle_abr_std']:.4f}\",\n            f\"{agg[accepts_abr_key]:.4f} +/- {agg[accepts_abr_std_key]:.4f}\",\n            f\"{agg['basl_abr_mean']:.4f} +/- {agg['basl_abr_std']:.4f}\",\n        ],\n    }\n    \n    df = pd.DataFrame(data)\n    df = df.set_index('Method')\n    \n    return df\n\n\n# Create and display Table 2\ntable_2 = create_table_2(exp2_data)\nif table_2 is not None:\n    print(\"\\n\" + \"=\"*70)\n    print(\"Table 2: BASL Training Effectiveness\")\n    print(\"=\"*70)\n    print(table_2.to_string())\n    print(\"=\"*70)\n    print(\"\\nInterpretation: BASL should outperform accepts-only and approach oracle.\")\n    print(\"Higher AUC = better discrimination. Lower ABR = better calibration.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Figure 4: Feature Bias Analysis\n",
    "\n",
    "Shows bias in predicted bad rates across the feature distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_figure_4(exp2_data: dict):\n    \"\"\"Plot Figure 4: Feature bias analysis.\"\"\"\n    \n    if not exp2_data or 'feature_bias' not in exp2_data:\n        print(\"Feature bias data not available\")\n        return None\n    \n    feature_bias = exp2_data['feature_bias']\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    for seed_data in feature_bias:\n        model_data = seed_data['model']\n        oracle_data = seed_data['oracle']\n        \n        # Panel 1: True vs Predicted bad rate\n        ax1 = axes[0]\n        bins = [d['bin'] for d in model_data]\n        \n        # Handle both old (x0_min/x0_max) and new (X1_min/X1_max) key names\n        if 'X1_min' in model_data[0]:\n            x1_centers = [(d['X1_min'] + d['X1_max']) / 2 for d in model_data]\n            x_label = 'Feature X1 (lower = lower risk)'\n        else:\n            x1_centers = [(d['x0_min'] + d['x0_max']) / 2 for d in model_data]\n            x_label = 'Feature x0 (lower = lower risk)'\n        \n        true_rates = [d['true_bad_rate'] for d in model_data]\n        model_pred = [d['predicted_bad_rate'] for d in model_data]\n        oracle_pred = [d['predicted_bad_rate'] for d in oracle_data]\n        \n        ax1.plot(x1_centers, true_rates, 'k-', linewidth=2.5, label='True bad rate', marker='o', markersize=6)\n        ax1.plot(x1_centers, model_pred, 'r--', linewidth=2, label='Model (BASL)', marker='s', markersize=5)\n        ax1.plot(x1_centers, oracle_pred, 'g:', linewidth=2, label='Oracle', marker='^', markersize=5)\n        \n        ax1.set_xlabel(x_label)\n        ax1.set_ylabel('Bad Rate')\n        ax1.set_title('True vs Predicted Bad Rate Across X1')\n        ax1.legend()\n        ax1.set_ylim(0, 1.1)\n        ax1.grid(True, alpha=0.3)\n        \n        # Panel 2: Bias across X1\n        ax2 = axes[1]\n        model_bias = [d['bias'] for d in model_data]\n        oracle_bias = [d['bias'] for d in oracle_data]\n        \n        width = 0.3\n        x_pos = np.array(x1_centers)\n        \n        ax2.bar(x_pos - width/2, model_bias, width=width, \n                label='Model bias', alpha=0.7, color='red')\n        ax2.bar(x_pos + width/2, oracle_bias, width=width, \n                label='Oracle bias', alpha=0.7, color='green')\n        ax2.axhline(y=0, color='k', linestyle='-', linewidth=1)\n        \n        ax2.set_xlabel(x_label)\n        ax2.set_ylabel('Bias (Predicted - True)')\n        ax2.set_title('Prediction Bias Across Feature Distribution')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n    \n    plt.suptitle('Figure 4: Feature Bias Analysis', fontsize=14, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    plt.show()\n    \n    return fig\n\n# Plot Figure 4\nfig4 = plot_figure_4(exp2_data)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Analysis: AUC Over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_auc_comparison(exp1_data: dict, exp2_data: dict = None):\n    \"\"\"Plot AUC comparison over iterations.\"\"\"\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Experiment 1: Evaluation\n    ax1 = axes[0]\n    if exp1_data and 'metrics_history' in exp1_data:\n        series = extract_metric_series(exp1_data['metrics_history'], 'auc')\n        \n        ax1.plot(series['iteration'], series['oracle'], 'k-', \n                 linewidth=2, label='Oracle', marker='o', markersize=3, markevery=10)\n        ax1.plot(series['iteration'], series['accepts'], 'r--', \n                 linewidth=2, label='Accepts-only', marker='s', markersize=3, markevery=10)\n        ax1.plot(series['iteration'], series['bayesian'], 'b-', \n                 linewidth=2, label='Bayesian', marker='^', markersize=3, markevery=10)\n        \n        ax1.set_xlabel('Iteration')\n        ax1.set_ylabel('AUC')\n        ax1.set_title('Experiment 1: AUC Evaluation')\n        ax1.legend(loc='best')\n        ax1.set_ylim(0.4, 1.0)\n        ax1.grid(True, alpha=0.3)\n    \n    # Experiment 2: Training\n    ax2 = axes[1]\n    if exp2_data and 'metrics_history' in exp2_data:\n        series = extract_metric_series(exp2_data['metrics_history'], 'auc')\n        \n        # Oracle line\n        ax2.plot(series['iteration'], series['oracle'], 'k-', \n                 linewidth=2, label='Oracle', marker='o', markersize=3, markevery=10)\n        \n        # Accepts-only baseline (constant reference from aggregated data)\n        if 'aggregated' in exp2_data:\n            agg = exp2_data['aggregated']\n            baseline_key = 'xgb_accepts_auc_mean' if 'xgb_accepts_auc_mean' in agg else 'accepts_auc_mean'\n            baseline_auc = agg[baseline_key]\n            ax2.axhline(y=baseline_auc, color='r', linestyle='--', \n                       linewidth=2, label='Accepts-only (baseline)')\n        \n        # BASL model evolution\n        if 'model_holdout' in series:\n            ax2.plot(series['iteration'], series['model_holdout'], 'b-', \n                     linewidth=2, label='BASL', marker='^', markersize=3, markevery=10)\n        \n        ax2.set_xlabel('Iteration')\n        ax2.set_ylabel('AUC')\n        ax2.set_title('Experiment 2: AUC Training')\n        ax2.legend(loc='best')\n        ax2.set_ylim(0.4, 1.0)\n        ax2.grid(True, alpha=0.3)\n    \n    plt.suptitle('AUC Over Iterations', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    return fig\n\n# Plot AUC comparison\nfig_auc = plot_auc_comparison(exp1_data, exp2_data)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def print_summary_stats(exp1_data: dict, exp2_data: dict):\n    \"\"\"Print summary statistics from experiments.\"\"\"\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"EXPERIMENT SUMMARY STATISTICS\")\n    print(\"=\"*70)\n    \n    # Experiment 1\n    if exp1_data and 'metrics_history' in exp1_data:\n        final = exp1_data['metrics_history'][-1]\n        print(\"\\nExperiment 1: Bayesian Evaluation (Final Iteration)\")\n        print(\"-\"*70)\n        print(f\"  Iteration: {final['iteration']}\")\n        print(f\"  Oracle ABR:     {final['oracle']['abr']:.4f}\")\n        print(f\"  Accepts ABR:    {final['accepts']['abr']:.4f}\")\n        print(f\"  Bayesian ABR:   {final['bayesian']['abr']:.4f}\")\n        \n        oracle_abr = final['oracle']['abr']\n        accepts_bias = abs(final['accepts']['abr'] - oracle_abr)\n        bayesian_bias = abs(final['bayesian']['abr'] - oracle_abr)\n        print(f\"\\n  Accepts bias (vs Oracle):   {accepts_bias:.4f}\")\n        print(f\"  Bayesian bias (vs Oracle):  {bayesian_bias:.4f}\")\n        if accepts_bias > 0:\n            improvement = (accepts_bias - bayesian_bias) / accepts_bias * 100\n            print(f\"  Bias reduction:             {improvement:.1f}%\")\n    \n    # Experiment 2\n    if exp2_data and 'aggregated' in exp2_data:\n        agg = exp2_data['aggregated']\n        \n        # Get baseline accepts-only metrics (constant reference)\n        baseline_auc_key = 'xgb_accepts_auc_mean' if 'xgb_accepts_auc_mean' in agg else 'accepts_auc_mean'\n        baseline_abr_key = 'xgb_accepts_abr_mean' if 'xgb_accepts_abr_mean' in agg else 'accepts_abr_mean'\n        \n        print(\"\\nExperiment 2: BASL Training (Final Model Performance)\")\n        print(\"-\"*70)\n        print(f\"  Oracle AUC:         {agg['oracle_auc_mean']:.4f}\")\n        print(f\"  Oracle ABR:         {agg['oracle_abr_mean']:.4f}\")\n        print(f\"  Accepts-only AUC:   {agg[baseline_auc_key]:.4f}\")\n        print(f\"  Accepts-only ABR:   {agg[baseline_abr_key]:.4f}\")\n        print(f\"  BASL AUC:           {agg['basl_auc_mean']:.4f}\")\n        print(f\"  BASL ABR:           {agg['basl_abr_mean']:.4f}\")\n        \n        # Calculate improvements\n        oracle_abr = agg['oracle_abr_mean']\n        accepts_gap = abs(agg[baseline_abr_key] - oracle_abr)\n        basl_gap = abs(agg['basl_abr_mean'] - oracle_abr)\n        \n        oracle_auc = agg['oracle_auc_mean']\n        accepts_auc_gap = abs(agg[baseline_auc_key] - oracle_auc)\n        basl_auc_gap = abs(agg['basl_auc_mean'] - oracle_auc)\n        \n        print(f\"\\n  ABR Gap to Oracle:\")\n        print(f\"    Accepts-only:     {accepts_gap:.4f} ({accepts_gap/oracle_abr*100:+.1f}%)\")\n        print(f\"    BASL:             {basl_gap:.4f} ({basl_gap/oracle_abr*100:+.1f}%)\")\n        if accepts_gap > 0:\n            abr_recovery = (accepts_gap - basl_gap) / accepts_gap * 100\n            print(f\"  ABR Recovery:       {abr_recovery:.1f}%\")\n        \n        print(f\"\\n  AUC Gap to Oracle:\")\n        print(f\"    Accepts-only:     {accepts_auc_gap:.4f}\")\n        print(f\"    BASL:             {basl_auc_gap:.4f}\")\n        if accepts_auc_gap > 0:\n            auc_recovery = (accepts_auc_gap - basl_auc_gap) / accepts_auc_gap * 100\n            print(f\"  AUC Recovery:       {auc_recovery:.1f}%\")\n    \n    print(\"\\n\" + \"=\"*70)\n\n\nprint_summary_stats(exp1_data, exp2_data)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interpretation Guide\n",
    "\n",
    "### Figure 2 Interpretation:\n",
    "- **Panel (a)**: Accepts and rejects have different distributions than population (covariate shift)\n",
    "- **Panel (b)**: Coefficients differ between accepts-only and oracle models (parameter bias)\n",
    "- **Panel (c)**: Score distributions differ (prediction bias)\n",
    "- **Panel (d)**: Bayesian evaluation tracks oracle better than accepts-only (evaluation bias reduction)\n",
    "- **Panel (e)**: BASL training reduces gap to oracle compared to accepts-only (training bias reduction)\n",
    "\n",
    "### Table 1 Interpretation:\n",
    "- Lower RMSE = better estimation of true performance\n",
    "- Bayesian should show lower RMSE than accepts-only across all metrics\n",
    "\n",
    "### Table 2 Interpretation:\n",
    "- BASL should achieve AUC/ABR closer to oracle than accepts-only\n",
    "- Higher AUC = better discrimination between good/bad\n",
    "- Lower ABR (closer to oracle) = better calibration\n",
    "\n",
    "### Figure 4 Interpretation:\n",
    "- Accepts-only model shows large bias in reject regions (negative bias)\n",
    "- Oracle model shows minimal bias across all regions\n",
    "- BASL (not shown) should reduce bias compared to accepts-only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}