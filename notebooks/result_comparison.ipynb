{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Comparison: Our Experiments vs Paper\n",
    "\n",
    "This notebook compares our experimental results with the reference values from the paper\n",
    "\"Fighting Sampling Bias: A Framework for Training and Evaluating Credit Scoring Models\".\n",
    "\n",
    "Key tables from the paper:\n",
    "- **Table C.3**: Experiment I - Evaluation accuracy (Bias, Variance, RMSE)\n",
    "- **Table C.4**: Experiment II - Loss due to bias and Gain from BASL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our experiment results\n",
    "experiment_dir = Path('../experiments/experiment_20251201_141716_100seeds_early_stopping')\n",
    "with open(experiment_dir / 'summary.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"Number of trials: {results['n_trials']}\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - n_periods: {results['config']['n_periods']}\")\n",
    "print(f\"  - batch_size: {results['config']['batch_size']}\")\n",
    "print(f\"  - accept_rate: {results['config']['accept_rate']}\")\n",
    "print(f\"  - bad_rate: {results['config']['bad_rate']}\")\n",
    "print(f\"  - holdout_size: {results['config']['holdout_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper reference values (Table C.4, Page A10)\n",
    "paper_table_c4 = {\n",
    "    'Metric': ['AUC', 'BS', 'PAUC', 'ABR', 'MMD'],\n",
    "    'Loss_due_to_bias': [0.0591, 0.0432, 0.0535, 0.0598, 0.5737],\n",
    "    'Gain_from_BASL_%': [35.72, 29.29, 22.42, 24.82, 3.74]\n",
    "}\n",
    "df_paper = pd.DataFrame(paper_table_c4)\n",
    "\n",
    "print(\"Paper Table C.4 - Experiment II Results:\")\n",
    "print(df_paper.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison table\n",
    "metrics = ['auc', 'pauc', 'brier', 'abr']\n",
    "metric_labels = ['AUC', 'PAUC', 'Brier Score', 'ABR']\n",
    "\n",
    "summary_data = []\n",
    "for metric, label in zip(metrics, metric_labels):\n",
    "    baseline = results['baseline'][metric]\n",
    "    basl = results['basl'][metric]\n",
    "    improvement = results['improvement'][metric]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Metric': label,\n",
    "        'Baseline Mean': baseline['mean'],\n",
    "        'Baseline Std': baseline['std'],\n",
    "        'BASL Mean': basl['mean'],\n",
    "        'BASL Std': basl['std'],\n",
    "        'Improvement Mean': improvement['mean'],\n",
    "        'Improvement Std': improvement['std'],\n",
    "        'Improvement %': (improvement['mean'] / baseline['mean']) * 100 if baseline['mean'] != 0 else 0\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"Our Experiment Results (100 seeds):\")\n",
    "print(df_summary.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate our \"Gain from BASL\" percentages for comparison with paper\n",
    "# Note: Paper defines Gain = (BASL improvement) / (Loss due to bias) * 100%\n",
    "# We only have BASL improvement, not the full oracle comparison\n",
    "\n",
    "comparison_data = []\n",
    "for metric, label in zip(metrics, metric_labels):\n",
    "    paper_row = df_paper[df_paper['Metric'] == label.replace('Brier Score', 'BS')]\n",
    "    if len(paper_row) > 0:\n",
    "        paper_loss = paper_row['Loss_due_to_bias'].values[0]\n",
    "        paper_gain = paper_row['Gain_from_BASL_%'].values[0]\n",
    "    else:\n",
    "        paper_loss = np.nan\n",
    "        paper_gain = np.nan\n",
    "    \n",
    "    our_improvement = results['improvement'][metric]['mean']\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Metric': label,\n",
    "        'Paper Loss due to Bias': paper_loss,\n",
    "        'Paper Gain from BASL (%)': paper_gain,\n",
    "        'Our Improvement': our_improvement,\n",
    "        'Our Estimated Gain (%)': (our_improvement / paper_loss * 100) if paper_loss and paper_loss != 0 else np.nan\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\nComparison with Paper Table C.4:\")\n",
    "print(df_comparison.round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline vs BASL Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side bar chart comparing baseline vs BASL\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    baseline = results['baseline'][metric]\n",
    "    basl = results['basl'][metric]\n",
    "    \n",
    "    # Data for bars\n",
    "    x = ['Baseline', 'BASL']\n",
    "    means = [baseline['mean'], basl['mean']]\n",
    "    stds = [baseline['std'], basl['std']]\n",
    "    \n",
    "    # Create bars\n",
    "    bars = ax.bar(x, means, yerr=stds, capsize=5, color=['#3498db', '#e74c3c'], alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01,\n",
    "                f'{mean:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    ax.set_title(label, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Value')\n",
    "    \n",
    "    # Add improvement annotation\n",
    "    improvement = results['improvement'][metric]['mean']\n",
    "    direction = '+' if improvement > 0 else ''\n",
    "    ax.annotate(f'{direction}{improvement:.4f}', \n",
    "                xy=(0.5, max(means)), \n",
    "                xytext=(0.5, max(means) * 1.15),\n",
    "                ha='center', fontsize=11,\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "plt.suptitle('Baseline vs BASL Performance (100 Seeds)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiment_dir / 'baseline_vs_basl_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improvement Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of improvements with confidence intervals\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    improvement = results['improvement'][metric]\n",
    "    \n",
    "    # Create horizontal box-style visualization using available stats\n",
    "    stats = [\n",
    "        improvement['min'],\n",
    "        improvement['q2.5'],\n",
    "        improvement['median'],\n",
    "        improvement['q97.5'],\n",
    "        improvement['max']\n",
    "    ]\n",
    "    \n",
    "    # Plot distribution range\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='No improvement')\n",
    "    ax.fill_between([0, 1], improvement['q2.5'], improvement['q97.5'], \n",
    "                    alpha=0.3, color='#3498db', label='95% CI')\n",
    "    ax.axhline(y=improvement['mean'], color='#e74c3c', linewidth=2, label=f\"Mean: {improvement['mean']:.4f}\")\n",
    "    ax.axhline(y=improvement['median'], color='#2ecc71', linewidth=2, linestyle='--', label=f\"Median: {improvement['median']:.4f}\")\n",
    "    \n",
    "    # Range bars\n",
    "    ax.axhline(y=improvement['min'], color='gray', linewidth=1, alpha=0.5)\n",
    "    ax.axhline(y=improvement['max'], color='gray', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    ax.set_title(f'{label} Improvement Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Improvement')\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_xticks([])\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    \n",
    "    # Add annotation for positive/negative regions\n",
    "    if improvement['q97.5'] > 0 and improvement['q2.5'] < 0:\n",
    "        ax.annotate('Mixed results\\n(some seeds worse)', xy=(0.7, improvement['q2.5']/2),\n",
    "                   fontsize=9, color='orange', ha='center')\n",
    "\n",
    "plt.suptitle('Distribution of Improvement (BASL - Baseline)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiment_dir / 'improvement_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed summary table similar to paper format\n",
    "detailed_summary = []\n",
    "\n",
    "for metric, label in zip(metrics, metric_labels):\n",
    "    for model_type, model_name in [('baseline', 'Baseline'), ('basl', 'BASL')]:\n",
    "        data = results[model_type][metric]\n",
    "        detailed_summary.append({\n",
    "            'Model': model_name,\n",
    "            'Metric': label,\n",
    "            'Mean': data['mean'],\n",
    "            'Std': data['std'],\n",
    "            'Median': data['median'],\n",
    "            'Q2.5%': data['q2.5'],\n",
    "            'Q97.5%': data['q97.5'],\n",
    "            'Min': data['min'],\n",
    "            'Max': data['max']\n",
    "        })\n",
    "\n",
    "df_detailed = pd.DataFrame(detailed_summary)\n",
    "print(\"Detailed Statistical Summary:\")\n",
    "print(df_detailed.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed summary to CSV\n",
    "df_detailed.to_csv(experiment_dir / 'detailed_summary.csv', index=False)\n",
    "print(f\"Saved to {experiment_dir / 'detailed_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Paper Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Filter to metrics we can compare\n",
    "comparable_metrics = ['AUC', 'PAUC', 'ABR']  # Exclude Brier and MMD\n",
    "paper_gains = [df_paper[df_paper['Metric'] == m]['Gain_from_BASL_%'].values[0] for m in comparable_metrics]\n",
    "\n",
    "# Calculate our estimated gains\n",
    "our_gains = []\n",
    "for m, paper_m in zip(['auc', 'pauc', 'abr'], comparable_metrics):\n",
    "    improvement = results['improvement'][m]['mean']\n",
    "    paper_loss = df_paper[df_paper['Metric'] == paper_m]['Loss_due_to_bias'].values[0]\n",
    "    our_gains.append((improvement / paper_loss * 100) if paper_loss != 0 else 0)\n",
    "\n",
    "x = np.arange(len(comparable_metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, paper_gains, width, label='Paper Gain (%)', color='#3498db', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, our_gains, width, label='Our Estimated Gain (%)', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Gain from BASL (%)')\n",
    "ax.set_title('Paper vs Our Results: Gain from BASL', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparable_metrics)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{bar.get_height():.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{bar.get_height():.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(experiment_dir / 'paper_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. AUC Results:\")\n",
    "auc_improvement = results['improvement']['auc']['mean']\n",
    "paper_auc_loss = 0.0591\n",
    "print(f\"   - Our improvement: {auc_improvement:.4f}\")\n",
    "print(f\"   - Paper loss due to bias: {paper_auc_loss}\")\n",
    "print(f\"   - Estimated recovery: {auc_improvement/paper_auc_loss*100:.1f}%\")\n",
    "print(f\"   - Paper expected: 35.72%\")\n",
    "print(f\"   - Status: {'Similar magnitude' if abs(auc_improvement/paper_auc_loss*100 - 35.72) < 20 else 'Different from paper'}\")\n",
    "\n",
    "print(\"\\n2. PAUC Results:\")\n",
    "pauc_improvement = results['improvement']['pauc']['mean']\n",
    "print(f\"   - Our improvement: {pauc_improvement:.4f}\")\n",
    "print(f\"   - Paper expected gain: 22.42%\")\n",
    "print(f\"   - Status: {'ISSUE - PAUC getting worse' if pauc_improvement < 0 else 'OK'}\")\n",
    "\n",
    "print(\"\\n3. ABR Results:\")\n",
    "abr_baseline = results['baseline']['abr']['mean']\n",
    "abr_basl = results['basl']['abr']['mean']\n",
    "print(f\"   - Our baseline ABR: {abr_baseline:.4f}\")\n",
    "print(f\"   - Our BASL ABR: {abr_basl:.4f}\")\n",
    "print(f\"   - Paper baseline ABR: ~0.20-0.22\")\n",
    "print(f\"   - Note: Our ABR values are much lower than paper\")\n",
    "\n",
    "print(\"\\n4. Holdout Bad Rate:\")\n",
    "holdout_br = results['holdout_bad_rate']['mean']\n",
    "print(f\"   - Mean: {holdout_br:.4f}\")\n",
    "print(f\"   - Expected: 0.70\")\n",
    "print(f\"   - Status: {'OK' if abs(holdout_br - 0.70) < 0.02 else 'Check data generation'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
