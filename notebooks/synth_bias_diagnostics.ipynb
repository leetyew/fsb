{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3408196",
   "metadata": {},
   "source": [
    "# Synthetic Bias Diagnostics (Acceptance Loop)\n",
    "Inspect whether selection bias **grows or stabilizes** during the acceptance loop, and summarize final exported datasets (`Da`, `Dr`, `H`).\n",
    "\n",
    "**Inputs**: a generated run folder containing:\n",
    "- `Da.csv`, `Dr.csv`, `H.csv`\n",
    "- `meta.json`\n",
    "- `snapshots/iter_*.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Set this to your generated run folder ----\n",
    "RUN_DIR = Path(\"synthetic_runs/REPLACE_WITH_RUN_ID\")  # e.g., synthetic_runs/2026-01-22_001\n",
    "assert RUN_DIR.exists(), f\"RUN_DIR not found: {RUN_DIR.resolve()}\"\n",
    "\n",
    "DA_PATH = RUN_DIR / \"Da.csv\"\n",
    "DR_PATH = RUN_DIR / \"Dr.csv\"\n",
    "H_PATH  = RUN_DIR / \"H.csv\"\n",
    "META_PATH = RUN_DIR / \"meta.json\"\n",
    "SNAP_DIR = RUN_DIR / \"snapshots\"\n",
    "\n",
    "for p in [DA_PATH, DR_PATH, H_PATH, META_PATH, SNAP_DIR]:\n",
    "    assert p.exists(), f\"Missing: {p}\"\n",
    "\n",
    "meta = json.loads(META_PATH.read_text())\n",
    "print(\"meta keys:\", sorted(meta.keys())[:20], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590dff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Da = pd.read_csv(DA_PATH)\n",
    "Dr = pd.read_csv(DR_PATH)\n",
    "H  = pd.read_csv(H_PATH)\n",
    "\n",
    "print(\"Da:\", Da.shape)\n",
    "print(\"Dr:\", Dr.shape)\n",
    "print(\"H :\", H.shape)\n",
    "\n",
    "assert \"y\" in Da.columns and \"y\" in H.columns, \"Da/H must contain y\"\n",
    "assert \"y\" not in Dr.columns, \"Dr must NOT contain y (unlabeled rejects)\"\n",
    "\n",
    "print(f\"Bad rate: Da={Da['y'].mean():.4f}, H={H['y'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load snapshot diagnostics\n",
    "snap_files = sorted(SNAP_DIR.glob(\"iter_*.json\"))\n",
    "assert len(snap_files) > 0, \"No snapshot json files found.\"\n",
    "\n",
    "snaps = [json.loads(f.read_text()) for f in snap_files]\n",
    "snap_df = pd.DataFrame(snaps).sort_values(\"iter\")\n",
    "\n",
    "snap_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: acceptance growth and bad rate over iterations\n",
    "plt.figure()\n",
    "plt.plot(snap_df[\"iter\"], snap_df[\"n_accepts\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cumulative accepts (n_accepts)\")\n",
    "plt.title(\"Acceptance growth over iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(snap_df[\"iter\"], snap_df[\"bad_rate_accepts\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Bad rate among accepts\")\n",
    "plt.title(\"Bad rate among accepts vs iteration\")\n",
    "plt.show()\n",
    "\n",
    "if \"auc_on_H\" in snap_df.columns:\n",
    "    plt.figure()\n",
    "    plt.plot(snap_df[\"iter\"], snap_df[\"auc_on_H\"])\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"AUC on H (loop model)\")\n",
    "    plt.title(\"Model AUC on H vs iteration (during loop)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift proxy: KS(Da vs H) per numeric feature (final snapshot)\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "feat_cols = [c for c in Da.columns if c != \"y\"]\n",
    "num_cols = [c for c in feat_cols if pd.api.types.is_numeric_dtype(Da[c])]\n",
    "\n",
    "ks_stats = []\n",
    "for c in num_cols:\n",
    "    a = Da[c].to_numpy()\n",
    "    h = H[c].to_numpy()\n",
    "    a = a[~np.isnan(a)]\n",
    "    h = h[~np.isnan(h)]\n",
    "    if len(a) == 0 or len(h) == 0:\n",
    "        continue\n",
    "    ks = ks_2samp(a, h).statistic\n",
    "    ks_stats.append((c, ks))\n",
    "\n",
    "ks_df = pd.DataFrame(ks_stats, columns=[\"feature\", \"ks_Da_vs_H\"]).sort_values(\"ks_Da_vs_H\", ascending=False)\n",
    "ks_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30800db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top drifting features (KS)\n",
    "top = ks_df.head(15)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(top)), top[\"ks_Da_vs_H\"].to_numpy())\n",
    "plt.xticks(range(len(top)), top[\"feature\"].to_list(), rotation=75, ha=\"right\")\n",
    "plt.ylabel(\"KS statistic (Da vs H)\")\n",
    "plt.title(\"Top 15 drifting features (final Da vs H)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Median KS:\", float(ks_df[\"ks_Da_vs_H\"].median()))\n",
    "print(\"Max KS:\", float(ks_df[\"ks_Da_vs_H\"].max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240a3e7",
   "metadata": {},
   "source": [
    "## Interpreting results\n",
    "- **n_accepts** should grow roughly linearly to your target (e.g., ~40k).\n",
    "- **bad_rate_accepts** should generally drop below **bad_rate(H)** due to selection.\n",
    "- **auc_on_H** (if tracked) often improves early then stabilizes.\n",
    "- **KS drift** should be non-trivial; late iterations often stabilize if the policy stabilizes.\n",
    "\n",
    "If things look unstable:\n",
    "- increase policy noise slightly (`Ïƒ_policy`)\n",
    "- increase initial seed accepts\n",
    "- reduce XGBoost complexity (depth/estimators) inside loop\n",
    "- reduce accept quota per iter\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
